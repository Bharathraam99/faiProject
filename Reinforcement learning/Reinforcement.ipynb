{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/supre/Documents/FAI/PROJECT /Users/fletcher/.cache/kagglehub/datasets/prodzar/stocks-historical-price-data/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"prodzar/stocks-historical-price-data\")\n",
    "\n",
    "print(\"C:/Users/supre/Documents/FAI/PROJECT\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged successfully into merged_stocks_data.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your directory containing the CSV files\n",
    "current_dir = os.getcwd()\n",
    "download_path = os.path.join(current_dir, '..', 'dataset', 'historical_data')\n",
    "download_path = os.path.normpath(download_path)\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for file in os.listdir(download_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # Extract company name from the filename (without .csv extension)\n",
    "        company_name = file.replace(\".csv\", \"\")\n",
    "        \n",
    "        # Load CSV into a DataFrame\n",
    "        file_path = os.path.join(download_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Add a new column for the company name\n",
    "        df[\"Company\"] = company_name\n",
    "        \n",
    "        # Append to the merged DataFrame\n",
    "        merged_data = pd.concat([merged_data, df], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "output_path = os.path.join(current_dir, '..', 'merged_stocks_data.csv')\n",
    "output_path = os.path.normpath(output_path)\n",
    "merged_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Data merged successfully into merged_stocks_data.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-31</td>\n",
       "      <td>271.279999</td>\n",
       "      <td>272.910004</td>\n",
       "      <td>268.750000</td>\n",
       "      <td>270.899994</td>\n",
       "      <td>2526600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>270.510010</td>\n",
       "      <td>272.809998</td>\n",
       "      <td>257.529999</td>\n",
       "      <td>259.029999</td>\n",
       "      <td>3903400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-06-04</td>\n",
       "      <td>263.269989</td>\n",
       "      <td>268.929993</td>\n",
       "      <td>257.459991</td>\n",
       "      <td>268.709991</td>\n",
       "      <td>3750800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-06-05</td>\n",
       "      <td>272.779999</td>\n",
       "      <td>273.209991</td>\n",
       "      <td>268.390015</td>\n",
       "      <td>272.859985</td>\n",
       "      <td>2650400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>273.720001</td>\n",
       "      <td>275.760010</td>\n",
       "      <td>271.049988</td>\n",
       "      <td>274.799988</td>\n",
       "      <td>2211800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Volume  \\\n",
       "0  2019-05-31  271.279999  272.910004  268.750000  270.899994  2526600   \n",
       "1  2019-06-03  270.510010  272.809998  257.529999  259.029999  3903400   \n",
       "2  2019-06-04  263.269989  268.929993  257.459991  268.709991  3750800   \n",
       "3  2019-06-05  272.779999  273.209991  268.390015  272.859985  2650400   \n",
       "4  2019-06-06  273.720001  275.760010  271.049988  274.799988  2211800   \n",
       "\n",
       "   Dividends  Stock Splits Company  \n",
       "0        0.0           0.0    ADBE  \n",
       "1        0.0           0.0    ADBE  \n",
       "2        0.0           0.0    ADBE  \n",
       "3        0.0           0.0    ADBE  \n",
       "4        0.0           0.0    ADBE  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import ta\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 205.20, Balance: 10205.20, Shares Held: 0, Actions: {'Buy_1': 101, 'Buy_5': 98, 'Hold': 98, 'Sell_5': 91, 'Sell_1': 88}\n",
      "Episode 2, Total Reward: 48.94, Balance: 10048.94, Shares Held: 0, Actions: {'Buy_1': 98, 'Sell_1': 98, 'Sell_5': 94, 'Buy_5': 93, 'Hold': 93}\n",
      "Episode 3, Total Reward: 44.45, Balance: 10044.45, Shares Held: 0, Actions: {'Sell_5': 108, 'Buy_5': 101, 'Hold': 92, 'Buy_1': 88, 'Sell_1': 87}\n",
      "Episode 4, Total Reward: 171.00, Balance: 10171.00, Shares Held: 0, Actions: {'Buy_1': 110, 'Buy_5': 99, 'Hold': 92, 'Sell_5': 91, 'Sell_1': 84}\n",
      "Episode 5, Total Reward: 164.39, Balance: 10164.39, Shares Held: 0, Actions: {'Hold': 120, 'Buy_5': 99, 'Sell_1': 88, 'Sell_5': 88, 'Buy_1': 81}\n",
      "Episode 6, Total Reward: 41.42, Balance: 10041.42, Shares Held: 0, Actions: {'Sell_5': 103, 'Hold': 98, 'Sell_1': 97, 'Buy_5': 95, 'Buy_1': 83}\n",
      "Episode 7, Total Reward: 84.60, Balance: 10084.60, Shares Held: 0, Actions: {'Sell_5': 102, 'Buy_1': 100, 'Hold': 95, 'Sell_1': 91, 'Buy_5': 88}\n",
      "Episode 8, Total Reward: 43.65, Balance: 10043.65, Shares Held: 0, Actions: {'Sell_5': 112, 'Sell_1': 106, 'Hold': 90, 'Buy_1': 88, 'Buy_5': 80}\n",
      "Episode 9, Total Reward: 62.91, Balance: 10062.91, Shares Held: 0, Actions: {'Hold': 117, 'Buy_1': 103, 'Sell_5': 97, 'Buy_5': 91, 'Sell_1': 68}\n",
      "Episode 10, Total Reward: 152.31, Balance: 10152.31, Shares Held: 0, Actions: {'Buy_5': 114, 'Buy_1': 102, 'Sell_5': 96, 'Hold': 86, 'Sell_1': 78}\n",
      "Episode 11, Total Reward: 174.33, Balance: 10174.33, Shares Held: 0, Actions: {'Buy_5': 109, 'Sell_5': 102, 'Buy_1': 92, 'Hold': 89, 'Sell_1': 84}\n",
      "Episode 12, Total Reward: 59.99, Balance: 10059.99, Shares Held: 0, Actions: {'Buy_1': 107, 'Hold': 100, 'Sell_5': 92, 'Sell_1': 92, 'Buy_5': 85}\n",
      "Episode 13, Total Reward: 64.34, Balance: 10064.34, Shares Held: 0, Actions: {'Sell_5': 111, 'Buy_5': 106, 'Hold': 92, 'Sell_1': 87, 'Buy_1': 80}\n",
      "Episode 14, Total Reward: 28.86, Balance: 10028.86, Shares Held: 0, Actions: {'Sell_5': 105, 'Sell_1': 99, 'Hold': 97, 'Buy_5': 92, 'Buy_1': 83}\n",
      "Episode 15, Total Reward: 100.24, Balance: 10100.24, Shares Held: 0, Actions: {'Sell_5': 105, 'Buy_5': 102, 'Hold': 99, 'Buy_1': 86, 'Sell_1': 84}\n",
      "Episode 16, Total Reward: 161.88, Balance: 10161.88, Shares Held: 0, Actions: {'Sell_5': 103, 'Sell_1': 100, 'Buy_5': 98, 'Buy_1': 90, 'Hold': 85}\n",
      "Episode 17, Total Reward: 123.31, Balance: 10123.31, Shares Held: 0, Actions: {'Hold': 108, 'Buy_5': 99, 'Sell_5': 94, 'Sell_1': 89, 'Buy_1': 86}\n",
      "Episode 18, Total Reward: 256.61, Balance: 10256.61, Shares Held: 0, Actions: {'Hold': 118, 'Buy_1': 98, 'Buy_5': 98, 'Sell_1': 84, 'Sell_5': 78}\n",
      "Episode 19, Total Reward: 190.77, Balance: 10190.77, Shares Held: 0, Actions: {'Hold': 117, 'Buy_5': 111, 'Sell_5': 84, 'Buy_1': 83, 'Sell_1': 81}\n",
      "Episode 20, Total Reward: 48.02, Balance: 10048.02, Shares Held: 0, Actions: {'Hold': 136, 'Sell_5': 99, 'Buy_5': 86, 'Buy_1': 86, 'Sell_1': 69}\n",
      "Episode 21, Total Reward: 213.69, Balance: 10213.69, Shares Held: 0, Actions: {'Hold': 111, 'Buy_5': 109, 'Buy_1': 90, 'Sell_5': 84, 'Sell_1': 82}\n",
      "Episode 22, Total Reward: 115.02, Balance: 10115.02, Shares Held: 0, Actions: {'Sell_5': 103, 'Buy_5': 100, 'Hold': 100, 'Sell_1': 92, 'Buy_1': 81}\n",
      "Episode 23, Total Reward: 54.00, Balance: 10054.00, Shares Held: 0, Actions: {'Hold': 119, 'Sell_5': 101, 'Buy_5': 96, 'Sell_1': 93, 'Buy_1': 67}\n",
      "Episode 24, Total Reward: 106.92, Balance: 10106.92, Shares Held: 0, Actions: {'Buy_1': 100, 'Buy_5': 99, 'Hold': 94, 'Sell_5': 92, 'Sell_1': 91}\n",
      "Episode 25, Total Reward: 294.15, Balance: 10294.15, Shares Held: 0, Actions: {'Hold': 118, 'Buy_5': 105, 'Sell_5': 94, 'Buy_1': 87, 'Sell_1': 72}\n",
      "Episode 26, Total Reward: 126.43, Balance: 10126.43, Shares Held: 0, Actions: {'Hold': 102, 'Buy_5': 101, 'Sell_5': 98, 'Buy_1': 90, 'Sell_1': 85}\n",
      "Episode 27, Total Reward: 1.41, Balance: 10001.41, Shares Held: 0, Actions: {'Sell_5': 113, 'Hold': 108, 'Buy_5': 95, 'Sell_1': 89, 'Buy_1': 71}\n",
      "Episode 28, Total Reward: 47.18, Balance: 10047.18, Shares Held: 0, Actions: {'Sell_5': 101, 'Hold': 101, 'Buy_5': 95, 'Sell_1': 90, 'Buy_1': 89}\n",
      "Episode 29, Total Reward: 89.64, Balance: 10089.64, Shares Held: 0, Actions: {'Hold': 119, 'Buy_5': 105, 'Sell_5': 101, 'Buy_1': 85, 'Sell_1': 66}\n",
      "Episode 30, Total Reward: 104.43, Balance: 10104.43, Shares Held: 0, Actions: {'Buy_5': 107, 'Hold': 103, 'Buy_1': 95, 'Sell_5': 92, 'Sell_1': 79}\n",
      "Episode 31, Total Reward: 230.62, Balance: 10230.62, Shares Held: 0, Actions: {'Hold': 111, 'Buy_5': 108, 'Sell_5': 96, 'Sell_1': 84, 'Buy_1': 77}\n",
      "Episode 32, Total Reward: 44.02, Balance: 10044.02, Shares Held: 0, Actions: {'Hold': 114, 'Sell_5': 109, 'Buy_1': 87, 'Buy_5': 86, 'Sell_1': 80}\n",
      "Episode 33, Total Reward: 102.59, Balance: 10102.59, Shares Held: 0, Actions: {'Hold': 111, 'Buy_5': 105, 'Sell_5': 101, 'Buy_1': 86, 'Sell_1': 73}\n",
      "Episode 34, Total Reward: 336.26, Balance: 10336.26, Shares Held: 0, Actions: {'Hold': 123, 'Buy_5': 123, 'Sell_5': 88, 'Buy_1': 74, 'Sell_1': 68}\n",
      "Episode 35, Total Reward: 44.92, Balance: 10044.92, Shares Held: 0, Actions: {'Hold': 105, 'Sell_5': 102, 'Buy_5': 98, 'Buy_1': 86, 'Sell_1': 85}\n",
      "Episode 36, Total Reward: 50.09, Balance: 10050.09, Shares Held: 0, Actions: {'Sell_5': 115, 'Hold': 113, 'Buy_5': 98, 'Sell_1': 75, 'Buy_1': 75}\n",
      "Episode 37, Total Reward: 80.74, Balance: 10080.74, Shares Held: 0, Actions: {'Hold': 118, 'Sell_5': 106, 'Buy_5': 100, 'Sell_1': 80, 'Buy_1': 72}\n",
      "Episode 38, Total Reward: 64.62, Balance: 10064.62, Shares Held: 0, Actions: {'Hold': 119, 'Sell_5': 101, 'Buy_5': 94, 'Sell_1': 85, 'Buy_1': 77}\n",
      "Episode 39, Total Reward: 226.88, Balance: 10226.88, Shares Held: 0, Actions: {'Hold': 126, 'Buy_5': 112, 'Sell_5': 88, 'Buy_1': 78, 'Sell_1': 72}\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to compute RSI using the ta library\n",
    "def compute_rsi(series, window=14):\n",
    "    return ta.momentum.RSIIndicator(series, window=window).rsi()\n",
    "\n",
    "# Define the base directory (adjust based on your project structure)\n",
    "base_dir = Path.cwd().parent  # Assuming the notebook is inside 'Reinforcement learning' folder\n",
    "\n",
    "# Define the relative path to the merged_stocks_data.csv\n",
    "data_path = base_dir / 'merged_stocks_data.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f\"The file {data_path} does not exist.\")\n",
    "\n",
    "# Load your merged data using the relative path\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Filter data by company name (e.g., for a specific company)\n",
    "company_data = data[data['Company'] == 'AAPL'].reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering: Adding Moving Averages and RSI as technical indicators\n",
    "company_data['MA10'] = company_data['Close'].rolling(window=10).mean()\n",
    "company_data['MA50'] = company_data['Close'].rolling(window=50).mean()\n",
    "company_data['RSI'] = compute_rsi(company_data['Close'], window=14)  # Using ta library\n",
    "\n",
    "# Discretize RSI into categories\n",
    "def discretize_rsi(rsi_value):\n",
    "    if rsi_value < 30:\n",
    "        return 'Oversold'\n",
    "    elif 30 <= rsi_value < 70:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Overbought'\n",
    "\n",
    "company_data['RSI_Category'] = company_data['RSI'].apply(discretize_rsi)\n",
    "\n",
    "# Drop rows with NaN values resulting from moving averages and RSI, then reset the index\n",
    "company_data = company_data.dropna().reset_index(drop=True)\n",
    "\n",
    "# Normalize the features (excluding categorical RSI)\n",
    "scaler = StandardScaler()\n",
    "company_data[['Open', 'High', 'Low', 'Close', 'Volume', 'MA10', 'MA50', 'RSI']] = scaler.fit_transform(\n",
    "    company_data[['Open', 'High', 'Low', 'Close', 'Volume', 'MA10', 'MA50', 'RSI']]\n",
    ")\n",
    "\n",
    "# Q-learning parameters\n",
    "actions = [\"Buy_1\", \"Buy_5\", \"Sell_1\", \"Sell_5\", \"Hold\"]  # Actions the agent can take\n",
    "alpha = 0.05  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "q_table = {}  # Q-table for state-action pairs\n",
    "\n",
    "# Initialize environment parameters\n",
    "initial_balance = 10000  # Increased initial balance for multiple shares\n",
    "balance = initial_balance  # Current balance\n",
    "shares_held = 0  # Initial shares held\n",
    "transaction_cost_percentage = 0.001  # 0.1% transaction cost per trade\n",
    "\n",
    "# Function to get state\n",
    "def get_state(data, t):\n",
    "    # Exclude categorical RSI for state representation\n",
    "    return tuple(data.iloc[t][[\"Close\", \"MA10\", \"MA50\", \"RSI\"]])\n",
    "\n",
    "# Reward function: Change in portfolio value\n",
    "def get_reward(action, current_price, shares_held, balance, previous_portfolio_value):\n",
    "    portfolio_value = balance + shares_held * current_price\n",
    "    # Implement actions\n",
    "    if action == \"Buy_1\" and balance >= (current_price * (1 + transaction_cost_percentage)):\n",
    "        cost = current_price * (1 + transaction_cost_percentage)\n",
    "        balance -= cost\n",
    "        shares_held += 1\n",
    "    elif action == \"Buy_5\" and balance >= (current_price * 5 * (1 + transaction_cost_percentage)):\n",
    "        cost = current_price * 5 * (1 + transaction_cost_percentage)\n",
    "        balance -= cost\n",
    "        shares_held += 5\n",
    "    elif action == \"Sell_1\" and shares_held >= 1:\n",
    "        revenue = current_price * (1 - transaction_cost_percentage)\n",
    "        balance += revenue\n",
    "        shares_held -= 1\n",
    "    elif action == \"Sell_5\" and shares_held >= 5:\n",
    "        revenue = current_price * 5 * (1 - transaction_cost_percentage)\n",
    "        balance += revenue\n",
    "        shares_held -= 5\n",
    "    # Hold does nothing\n",
    "    \n",
    "    new_portfolio_value = balance + shares_held * current_price\n",
    "    reward = new_portfolio_value - previous_portfolio_value\n",
    "    return reward, balance, shares_held\n",
    "\n",
    "# Initialize a list to store rewards for visualization\n",
    "episode_rewards = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(1, 101):  # Training for 100 episodes\n",
    "    balance = initial_balance\n",
    "    shares_held = 0\n",
    "    total_reward = 0\n",
    "    actions_taken = []\n",
    "    \n",
    "    for t in range(len(company_data) - 1):\n",
    "        state = get_state(company_data, t)\n",
    "        \n",
    "        # Initialize Q-values for unseen states\n",
    "        if state not in q_table:\n",
    "            q_table[state] = [0] * len(actions)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_index = random.randint(0, len(actions) - 1)  # Explore\n",
    "        else:\n",
    "            action_index = np.argmax(q_table[state])  # Exploit\n",
    "        \n",
    "        action = actions[action_index]\n",
    "        current_price = company_data.loc[t, \"Close\"]\n",
    "        previous_portfolio_value = balance + shares_held * current_price\n",
    "        reward, balance, shares_held = get_reward(action, current_price, shares_held, balance, previous_portfolio_value)\n",
    "        \n",
    "        actions_taken.append(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Next state\n",
    "        next_state = get_state(company_data, t + 1)\n",
    "        \n",
    "        # Initialize Q-values for unseen next states\n",
    "        if next_state not in q_table:\n",
    "            q_table[next_state] = [0] * len(actions)\n",
    "        \n",
    "        # Q-learning update using Bellman equation\n",
    "        old_q_value = q_table[state][action_index]\n",
    "        next_max_q = max(q_table[next_state])\n",
    "        q_table[state][action_index] = old_q_value + alpha * (reward + gamma * next_max_q - old_q_value)\n",
    "    \n",
    "    # Handle end-of-episode: sell any remaining shares\n",
    "    final_price = company_data.loc[len(company_data) - 1, \"Close\"]\n",
    "    if shares_held > 0:\n",
    "        revenue = final_price * (1 - transaction_cost_percentage) * shares_held\n",
    "        balance += revenue\n",
    "        total_reward += revenue\n",
    "        shares_held = 0\n",
    "    \n",
    "    # Calculate final portfolio value\n",
    "    final_portfolio_value = balance + shares_held * final_price\n",
    "    episode_reward = final_portfolio_value - initial_balance\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    # Logging\n",
    "    action_counts = pd.Series(actions_taken).value_counts().to_dict()\n",
    "    print(f\"Episode {episode}, Total Reward: {episode_reward:.2f}, Balance: {balance:.2f}, Shares Held: {shares_held}, Actions: {action_counts}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Plot Training Rewards\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 101), episode_rewards, marker='o')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Training Progress: Total Reward per Episode')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save the Q-table for future use\n",
    "import pickle\n",
    "with open(base_dir / 'q_table.pkl', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n",
    "print(\"Q-table saved successfully.\")\n",
    "\n",
    "# Test phase (using the trained Q-table)\n",
    "balance = initial_balance\n",
    "shares_held = 0\n",
    "total_reward = 0\n",
    "\n",
    "for t in range(len(company_data) - 1):\n",
    "    state = get_state(company_data, t)\n",
    "    if state not in q_table:\n",
    "        q_table[state] = [0] * len(actions)\n",
    "    \n",
    "    action_index = np.argmax(q_table[state])  # Always exploit\n",
    "    action = actions[action_index]\n",
    "    current_price = company_data.loc[t, \"Close\"]\n",
    "    previous_portfolio_value = balance + shares_held * current_price\n",
    "    reward, balance, shares_held = get_reward(action, current_price, shares_held, balance, previous_portfolio_value)\n",
    "    total_reward += reward\n",
    "\n",
    "# Sell any remaining shares at the end of the test\n",
    "final_price = company_data.loc[len(company_data) - 1, \"Close\"]\n",
    "if shares_held > 0:\n",
    "    revenue = final_price * (1 - transaction_cost_percentage) * shares_held\n",
    "    balance += revenue\n",
    "    total_reward += revenue\n",
    "    shares_held = 0\n",
    "\n",
    "# Calculate final portfolio value\n",
    "final_portfolio_value = balance + shares_held * final_price\n",
    "test_reward = final_portfolio_value - initial_balance\n",
    "\n",
    "print(f\"Test Total Reward: {test_reward:.2f}, Final Balance: {balance:.2f}, Shares Held: {shares_held}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State and Q-Table: For each day in the test data, the agent retrieves the current state and checks for that state in the Q-table.\n",
    "\n",
    "Exploitation (No Exploration): For each state in the test data, the agent selects the action with the highest Q-value.\n",
    "\n",
    "Action Execution:\n",
    "\"Buy\": Deduct the price of one stock from the balance and increase the shares_held.\n",
    "\"Sell\": Add the value of all held shares to the balance and set shares_held to zero.\n",
    "\n",
    "Portfolio Value Calculation: Tracks the total portfolio value (balance + value of held shares) to see the agent’s progress.\n",
    "\n",
    "End of Testing: Prints the final portfolio value and total profit at the end of the test period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
